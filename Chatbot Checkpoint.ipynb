{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9aa0b2-5342-487c-a4d6-81b48efc59be",
   "metadata": {},
   "source": [
    "In this exercise, you will learn how to create your own chatbot by modifying the code provided in the example in our course. You will need to choose a topic that you want your chatbot to be based on and find a text file related to that topic. Then, you will need to modify the code to preprocess the data in the text file and create a chatbot interface that can interact with the user.\n",
    "\n",
    "\n",
    "Instructions\n",
    "\n",
    "Choose a topic: Choose a topic that you are interested in and find a text file related to that topic. You can use websites such as Project Gutenberg to find free text files.\n",
    "Preprocess the data: Modify the preprocess() function in the code provided to preprocess the data in your text file. You may want to modify the stop words list or add additional preprocessing steps to better suit your needs.\n",
    "Define the similarity function: Modify the get_most_relevant_sentence() function to compute the similarity between the user's query and each sentence in your text file. You may want to modify the similarity metric or add additional features to improve the performance of your chatbot.\n",
    "Define the chatbot function: Modify the chatbot() function to return an appropriate response based on the most relevant sentence in your text file.\n",
    "Create a Streamlit app: Use the main() function in the code provided as a template to create a web-based chatbot interface. Prompt the user for a question, call the chatbot() function to get the response, and display it on the screen.\n",
    "Note:\n",
    "\n",
    "To run your code, you need to have the text file in the same directory as your Python script.\n",
    "You may want to test your chatbot with different types of questions to ensure that it is working correctly.\n",
    "You can continue to modify your chatbot to add additional features or improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42cab12a-308c-4e02-b127-1d357196e7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Special\n",
      "[nltk_data]     User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Special User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf05931-d2b7-4641-aafd-85331eef4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text file\n",
    "with open('alice_in_wonderland.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ee9782-acc5-440b-ae97-1c02d2e6f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10cd4a5c-851b-46de-9840-bdb02270d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess each sentence\n",
    "def preprocess(sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Tokenize text into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "corpus = [preprocess(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c54a392-dca9-4999-aad0-93494248ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(query, sentence):\n",
    "    query_set = set(preprocess(query))\n",
    "    sentence_set = set(sentence)\n",
    "    return len(query_set.intersection(sentence_set)) / len(query_set.union(sentence_set))\n",
    "\n",
    "def get_response(query):\n",
    "    max_similarity = 0\n",
    "    best_response = \"\"\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        similarity = jaccard_similarity(query, sentence)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            #best_response = \" \".join(sentence)\n",
    "            best_response = sentences[i] # Use the original sentence with stopwords included\n",
    "    return best_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ac5f53-7173-4200-8026-01bc96cecd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poor Alice!\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "user_query = \" How does Alice enter Wonderland?\"\n",
    "response = get_response(user_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b67b2f3-7e29-4167-a3ef-bfceae7433af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The White Rabbit put on his spectacles.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Why did Alice follow the White Rabbit?\"\n",
    "response = get_response(user_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0249d075-3fef-48b6-a83f-3236f855e41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type in 'quit' to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(\"Type in 'quit' to quit\")\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "    response = get_response(user_input)\n",
    "    print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f2e6ad-a6ea-4938-8dab-d860d051ecc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatbot_wonderland.py creation executed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the file chatbot_wonderland.py in write mode\n",
    "with open(\"chatbot_wonderland.py\", \"w\") as file:\n",
    "    # Writing the Streamlit code into the file\n",
    "    file.write('''\n",
    "    \n",
    "##### Let's build a beginner-friendly chatbot in Streamlit #####\n",
    "# This project will build a chatbot that reads a text file, processes it, and returns relevant answers based on user input.\n",
    "\n",
    "# Importing necessary libraries\n",
    "\n",
    "# nltk (Natural Language Toolkit) library for various text processing tasks\n",
    "import nltk\n",
    "import streamlit as st  # Streamlit is used for building interactive web applications\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Tokenizers for splitting text into words and sentences\n",
    "from nltk.corpus import stopwords  # List of common words (stopwords) that are usually removed from text (like \"is\", \"the\", \"and\")\n",
    "from nltk.stem import WordNetLemmatizer  # Lemmatizer to reduce words to their base form (e.g., 'running' -> 'run')\n",
    "import string  # Python's built-in library for handling strings and punctuation\n",
    "\n",
    "# Uncomment to download necessary NLTK resources if not downloaded already\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# Ensure the necessary NLTK data is available\n",
    "# Set a custom NLTK data directory\n",
    "nltk_data_dir = os.path.join(os.getcwd(), 'nltk_data')\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "\n",
    "# Point NLTK to the custom directory\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "# Load stopwords and initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))  # Load a set of common English stopwords to filter out later\n",
    "lemmatizer = WordNetLemmatizer()  # Initialize a lemmatizer to reduce words to their base form\n",
    "\n",
    "# Define a function to preprocess text (tokenizing, removing stopwords and punctuation, lemmatizing)\n",
    "def preprocess(sentence):\n",
    "    # Tokenize the sentence into words and convert to lowercase\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Remove stopwords and punctuation from the list of words\n",
    "    words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
    "    \n",
    "    # Lemmatize each word to convert it to its base form (e.g., 'running' -> 'run')\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Return the list of processed words\n",
    "    return words\n",
    "\n",
    "# Load the text file (Alice in Wonderland)\n",
    "def load_text():\n",
    "    try:\n",
    "        # Provide the path to the text file\n",
    "        file_path = r'C:\\\\Users\\\\Special User\\\\Downloads\\\\alice_in_wonderland.txt'\n",
    "        \n",
    "        # Open the file, read its content, and replace newline characters with spaces\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read().replace('\\\\n', ' ')\n",
    "    \n",
    "    # Handle case where the file is not found and display an error message in Streamlit\n",
    "    except FileNotFoundError:\n",
    "        st.error(\"Text file not found.\")\n",
    "        return \"\"\n",
    "# Tokenize the text into sentences and preprocess them\n",
    "def prepare_corpus(text):\n",
    "    # Tokenize the text into individual sentences using sent_tokenize\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Preprocess each sentence (tokenizing, removing stopwords/punctuation, and lemmatizing)\n",
    "    return [preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "# Calculate Jaccard similarity between two sets\n",
    "def jaccard_similarity(query, sentence):\n",
    "    # Convert both the query and sentence to sets (unique words)\n",
    "    query_set = set(query)\n",
    "    sentence_set = set(sentence)\n",
    "    \n",
    "    # If the union of both sets is zero, return 0 to avoid division by zero\n",
    "    if len(query_set.union(sentence_set)) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate the Jaccard similarity as the size of intersection divided by the size of union\n",
    "    return len(query_set.intersection(sentence_set)) / len(query_set.union(sentence_set))\n",
    "\n",
    "# Find the most relevant sentence using Jaccard similarity\n",
    "def get_most_relevant_sentence(query, corpus, original_sentences):\n",
    "    # Preprocess the user query (tokenization, stopword removal, etc.)\n",
    "    query = preprocess(query)\n",
    "    \n",
    "    # Initialize variables to store the maximum similarity and best matching sentence\n",
    "    max_similarity = 0\n",
    "    best_sentence = \"I couldn't find a relevant answer.\"  # Default response if no match is found\n",
    "    \n",
    "    # Iterate over the corpus of preprocessed sentences to find the best match\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        # Calculate the Jaccard similarity between the user query and the current sentence\n",
    "        similarity = jaccard_similarity(query, sentence)\n",
    "        \n",
    "        # If the similarity score is higher than the current maximum, update the best sentence\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_sentence = original_sentences[i]  # Retrieve the original sentence (before preprocessing)\n",
    "    \n",
    "    # Return the most relevant sentence (or the default response if no match is found)\n",
    "    return best_sentence\n",
    "\n",
    "# Main function to create the chatbot interface in Streamlit\n",
    "def main():\n",
    "    # Title for the app\n",
    "    st.title(\"Wonderland's Novice Chatbot\")\n",
    "    \n",
    "    # A brief description of the chatbot's purpose\n",
    "    st.write(\"Hello! Ask me anything related to Alice in Wonderland!\")\n",
    "    \n",
    "    # Add a dropdown (expander) for suggested questions\n",
    "    with st.expander(\"Click me for suggestions\"):\n",
    "        st.write(\"\"\"\n",
    "        1. Who does Alice meet first in Wonderland?\n",
    "        2. What is the Cheshire Cats famous line?\n",
    "        3. How does Alice enter Wonderland?\n",
    "        4. What is the Queen of Hearts known for?\n",
    "        5. Why did Alice follow the White Rabbit?\n",
    "        6. What was Alice reaction to the Mad Hatter tea party?\n",
    "        7. What advice does the Caterpillar give Alice?\n",
    "        8. What is the significance of the bottle labeled 'Drink Me'?\n",
    "        9. How does the story of Alice in Wonderland end?\n",
    "        10. What game does the Queen of Hearts play with Alice?\n",
    "        \"\"\")\n",
    "    # Load and prepare text corpus\n",
    "    text = load_text()  # Load the text from the file (Alice in Wonderland)\n",
    "    if text:\n",
    "        # Preprocess the text to create a corpus of tokenized sentences\n",
    "        corpus = prepare_corpus(text)  # Prepares the text into a list of preprocessed sentences\n",
    "        original_sentences = sent_tokenize(text)  # Tokenizes the original text into sentences for later reference\n",
    "\n",
    "        # Get user input from the Streamlit interface\n",
    "        user_input = st.text_input(\"Enter your question:\")  # Input field for the user's question\n",
    "\n",
    "        # If the user clicks the submit button\n",
    "        if st.button(\"Submit\"):\n",
    "            if user_input:\n",
    "                # Get the most relevant sentence from the corpus based on the user's input\n",
    "                response = get_most_relevant_sentence(user_input, corpus, original_sentences)\n",
    "                st.write(f\"Chatbot: {response}\")  # Display the chatbot's response\n",
    "            else:\n",
    "                st.write(\"Please enter a question.\")  # Prompt user to enter a question if the input is empty\n",
    "                # Run the Streamlit app\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Call the main function to run the Streamlit app\n",
    "    ''')\n",
    "\n",
    "print(\"chatbot_wonderland.py creation executed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1674891a-4036-4629-9e3d-432ee81d5f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
